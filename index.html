
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
        <link rel="next" href="compare/">
      
      
      <link rel="icon" href="img/logo_v2.svg">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.33">
    
    
      
        <title>MLPerf Inference Results Comparison</title>
      
    
    
  
      <link rel="stylesheet" href="assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/jquery.tablesorter.pager.min.css">
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/theme.blackice.min.css">
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/theme.blue.min.css">
<style type="text/css">

table.resultstable, table.counttable {
    overflow-x: auto;
}

.pager1{
    display: none!important;
}

.pagerSavedHeightSpacer {
    display: none!important;
}
.resultstable_wrapper, .counttable_wrapper {
    overflow-x: auto;
}

/* General form styling */
form {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background-color: #f5f5f5;
    border-radius: 8px;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
}

/* Style for form group */
.form-group {
    margin-bottom: 20px;
    position: relative;
}

/* Label styling */
label {
    display: block;
    font-size: 14px;
    color: #333;
    margin-bottom: 5px;
    font-weight: 500;
}

/* Select styling */
select {
    width: 100%;
    padding: 10px;
    font-size: 16px;
    color: #333;
    background-color: #fff;
    border: 1px solid #ccc;
    border-radius: 4px;
    box-shadow: none;
    appearance: none;
    transition: border-color 0.3s ease;
}

select:focus {
    border-color: #6200ee;
    outline: none;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

/* Custom arrow for select */
select::-ms-expand {
    display: none;
}

.select-wrapper {
    position: relative;
}

.select-wrapper::after {
    content: '';
    position: absolute;
    top: 50%;
    right: 10px;
    width: 0;
    height: 0;
    border-left: 5px solid transparent;
    border-right: 5px solid transparent;
    border-top: 5px solid #333;
    pointer-events: none;
    transform: translateY(-50%);
}

/* Button styling */
button {
    width: 100%;
    padding: 12px;
    font-size: 16px;
    color: #fff;
    background-color: #6200ee;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background-color 0.3s ease;
}

button:hover {
    background-color: #3700b3;
}

button:active {
    background-color: #6200ee;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
}

button:disabled {
    background-color: #ccc;
    cursor: not-allowed;
}

/* Input and button ripple effect */
.button-ripple, .input-ripple {
    position: relative;
    overflow: hidden;
}

.button-ripple::after, .input-ripple::after {
    content: '';
    position: absolute;
    top: 50%;
    left: 50%;
    width: 100px;
    height: 100px;
    background: rgba(255, 255, 255, 0.4);
    border-radius: 50%;
    transform: translate(-50%, -50%) scale(0);
    transition: transform 0.5s ease, opacity 1s ease;
    opacity: 0;
    pointer-events: none;
}

.button-ripple:active::after, .input-ripple:focus::after {
    transform: translate(-50%, -50%) scale(1);
    opacity: 1;
    transition: 0s;
}

/* Responsive Design */
@media (max-width: 768px) {
    form {
        max-width: 100%;
        padding: 15px;
    }

    button {
        padding: 10px;
        font-size: 14px;
    }

    select {
        font-size: 14px;
        padding: 8px;
    }
}

@media (max-width: 480px) {
    form {
        padding: 10px;
    }

    button {
        padding: 8px;
        font-size: 14px;
    }

    select {
        font-size: 14px;
        padding: 8px;
    }

    label {
        font-size: 13px;
    }
}

select.pagesize, select.gotoPage {
    width: fit-content;
    padding: 5px;
}


/*Testing*/
/* Base Table Styles */
#results_table {
    margin: 20px 0;
    overflow-x: auto;
}

.tablesorter {
    width: 100%;
    border-collapse: collapse;
    background-color: #fff;
    font-size: 14px;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    overflow: hidden;
}

/* Header and Footer Styles */
.tablesorter thead th,
.tablesorter tfoot th {
    background-color: #6200ea; /* Material Design Purple */
    color: #ffffff;
    padding: 12px;
    text-align: left;
    text-transform: uppercase;
    font-weight: 500;
    border-bottom: 2px solid #512da8; /* Darker purple for bottom border */
}

.tablesorter tfoot th {
    background-color: #6200ea;
}

/* Body Row Styles */
.tablesorter tbody tr {
    border-bottom: 1px solid #e0e0e0;
    transition: background-color 0.3s ease;
}

.tablesorter tbody tr:nth-child(even) {
    background-color: #f9f9f9;
}

.tablesorter tbody tr:hover {
    background-color: #eeeeee;
}

.tablesorter tbody td {
    padding: 12px;
    text-align: left;
    color: #424242;
}

/* Specific Column Styles */
#col-id,
#col-system,
#col-submitter,
#col-accelerator {
    font-weight: bold;
}
#col-id, .col-id {
    min-width: 80px;
    max-width: 80px;
    width: 80px;
    left: 0px;
}
td.col-result {
    min-width: 100px;
    text-align: right!important;
}
td.col-system, th.col-system {
    min-width: 150px;
    max-width: 150px;
    width: 150px;
    word-break: break-word;
    left: 80px;
}
td.col-submitter, th.col-submitter {
    min-width: 120px;
    max-width: 120px;
    width: 120px;
    word-break: break-word;
    left: 230px;
}
td.col-accelerator, th.col-accelerator {
    min-width: 120px;
    max-width: 120px;
    width: 120px;
    word-break: break-word;
    left: 350px;
}

td.count-submitter, th.count-submitter {
    min-width: 150px;
    max-width: 150px;
    width: 150px;
    left: 0px;
    background: white;
    font-weight: 600;
    font-size: Large;
    word-break: break-word;
    position:sticky;
}
.col-scenario {
    font-weight: 500;
    color: #6200ea;
    text-align: center;
}

.col-result {
    text-align: right;
    color: #303f9f; /* Material Design Blue */
    font-weight: 600;
}

.headcol {
  position: sticky;
  background:white;
  border:none!important;
}

.collapsible {
    color: white;
    cursor: pointer;
    padding: 10px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
}

.chart {
    /* padding: 0 18px; */
    display: none; /* Hidden by default */
    overflow: hidden;
    height: 500px;
    width: 100%;
    background-color: white;
}


/* Responsive Design */
@media (max-width: 768px) {
    .tablesorter thead,
    .tablesorter tfoot {
        display: none;
    }

    .tablesorter tbody tr {
        display: block;
        margin-bottom: 15px;
        border: 1px solid #ddd;
        border-radius: 8px;
    }

    .tablesorter tbody td {
        display: flex;
        justify-content: space-between;
        padding: 10px;
        text-align: right;
    }

    .tablesorter tbody td::before {
        content: attr(data-label);
        font-weight: bold;
        color: #6200ea;
        text-transform: uppercase;
    }

    .tablesorter tbody td:first-child {
        border-top-left-radius: 8px;
        border-bottom-left-radius: 8px;
    }

    .tablesorter tbody td:last-child {
        border-top-right-radius: 8px;
        border-bottom-right-radius: 8px;
    }
}



</style>

  <!-- Add scripts that need to run afterwards here -->

    
  
      
    
<!-- load jQuery and tablesorter scripts -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>


<script type="text/javascript" src="thirdparty/tablesorter/dist/js/jquery.tablesorter.js"></script>
<!-- tablesorter widgets (optional) -->

<script type="text/javascript" src="thirdparty/tablesorter/dist/js/jquery.tablesorter.widgets.js"></script>
<script type="text/javascript" src="thirdparty/tablesorter/dist/js/extras/jquery.tablesorter.pager.min.js"></script>
  <!-- Add scripts that need to run before here -->
  <script src="https://cdn.canvasjs.com/ga/jquery.canvasjs.min.js"></script>
  
<script type="text/javascript" src="javascripts/common.js"></script>
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="yellow">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="MLPerf Inference Results Comparison" class="md-header__button md-logo" aria-label="MLPerf Inference Results Comparison" data-md-component="logo">
      
  <img src="img/logo_v2.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MLPerf Inference Results Comparison
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Results
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/mlcommons/inference_results_v4.0" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="." class="md-tabs__link">
        
  
    
  
  Results

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="compare/" class="md-tabs__link">
        
  
    
  
  Compare

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="top_results/" class="md-tabs__link">
        
  
    
  
  Top Results

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="MLPerf Inference Results Comparison" class="md-nav__button md-logo" aria-label="MLPerf Inference Results Comparison" data-md-component="logo">
      
  <img src="img/logo_v2.svg" alt="logo">

    </a>
    MLPerf Inference Results Comparison
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mlcommons/inference_results_v4.0" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Results
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="compare/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Compare
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="top_results/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Top Results
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Results</h1>

<html>

        <h2 id="results_heading_available" class="results_table_heading">Datacenter Category: Available submissions in Closed division</h2>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<div id="results_table_available" class="resultstable_wrapper"> <table class="resultstable tablesorter tableclosed tabledatacenter" id="results_available"><thead> <tr>
        <th id="col-id" class="headcol col-id">ID</th>
        <th id="col-system" class="headcol col-system">System</th>
        <th id="col-submitter" class="headcol col-submitter">Submitter</th>
        <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
        <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
        <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
        <th id="col-gptj-99" colspan="2">GPTJ-99</th>
        <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
        <th id="col-bert-99" colspan="2">Bert-99</th>
        <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
        <th id="col-dlrm-v2-99" colspan="2">Stable Diffusion</th>
        <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
        <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
        <th id="col-retinanet" colspan="2">Retinanet</th>
        <th id="col-resnet50" colspan="2">ResNet50</th>
        <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
        <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
        </tr>
    <tr>
    <th class="headcol col-id"></th>
    <th class="headcol col-system"></th>
    <th class="headcol col-submitter"></th>
    <th class="headcol col-accelerator"></th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Offline</th>
    </tr></thead><tfoot> <tr>
        <th id="col-id" class="headcol col-id">ID</th>
        <th id="col-system" class="headcol col-system">System</th>
        <th id="col-submitter" class="headcol col-submitter">Submitter</th>
        <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
        <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
        <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
        <th id="col-gptj-99" colspan="2">GPTJ-99</th>
        <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
        <th id="col-bert-99" colspan="2">Bert-99</th>
        <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
        <th id="col-dlrm-v2-99" colspan="2">Stable Diffusion</th>
        <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
        <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
        <th id="col-retinanet" colspan="2">Retinanet</th>
        <th id="col-resnet50" colspan="2">ResNet50</th>
        <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
        <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
        </tr>
    <tr>
    <th class="headcol col-id"></th>
    <th class="headcol col-system"></th>
    <th class="headcol col-submitter"></th>
    <th class="headcol col-accelerator"></th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Offline</th>
    </tr></tfoot>
        <tr>
        <td class="col-id headcol"> 4.0-0001 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/systems/ESC8000_E11P_H100x8_TRT.json"> ESC8000-E11P (8x H100-PCIe-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> NVIDIA H100-PCIe-80GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/bert-99/Offline"> 35349.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/bert-99/Offline"> 45227.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/bert-99.9/Offline"> 31993.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/bert-99.9/Offline"> 38577.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/stable-diffusion-xl/Offline"> 7.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/stable-diffusion-xl/Offline"> 8.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/dlrm-v2-99/Offline"> 279993.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/dlrm-v2-99/Offline"> 336637.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/dlrm-v2-99.9/Offline"> 199999.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/dlrm-v2-99.9/Offline"> 201726.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/retinanet/Offline"> 8394.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/retinanet/Offline"> 9286.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/resnet50/Offline"> 368024.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/resnet50/Offline"> 445908.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/3d-unet-99/Offline"> 37.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11P_H100x8_TRT/3d-unet-99.9/Offline"> 37.2 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0002 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Gold 6442Y
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 24
Processors per node: 2
Nodes: 1
Notes: Data bandwidth for GPU-PCIe: 252 GB/s; PCIe-NIC: 63 GB/s
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/systems/ESC8000_E11_L40Sx8_TRT.json"> ESC8000-E11 (8x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/gptj-99/Offline"> 97.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/gptj-99/Offline"> 99.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/gptj-99.9/Offline"> 97.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/gptj-99.9/Offline"> 98.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/bert-99/Offline"> 25120.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/bert-99/Offline"> 26077.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/bert-99.9/Offline"> 11995.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/bert-99.9/Offline"> 13709.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/stable-diffusion-xl/Offline"> 5.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/stable-diffusion-xl/Offline"> 5.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/dlrm-v2-99/Offline"> 179985.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/dlrm-v2-99/Offline"> 196999.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/dlrm-v2-99.9/Offline"> 94969.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/dlrm-v2-99.9/Offline"> 101691.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/retinanet/Offline"> 5797.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/retinanet/Offline"> 6401.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/resnet50/Offline"> 355029.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/resnet50/Offline"> 369341.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/3d-unet-99/Offline"> 31.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L40Sx8_TRT/3d-unet-99.9/Offline"> 31.1 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0003 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Gold 6442Y
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 24
Processors per node: 2
Nodes: 1
Notes: Data bandwidth for GPU-PCIe: 252 GB/s; PCIe-NIC: 15.7 GB/s
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/systems/ESC8000_E11_L4x8_TRT.json"> ESC8000-E11 (8x L4, TensorRT) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> NVIDIA L4 x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L4x8_TRT/bert-99/Offline"> 7305.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L4x8_TRT/bert-99/Offline"> 7597.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L4x8_TRT/bert-99.9/Offline"> 5008.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/ASUSTeK/results/ESC8000_E11_L4x8_TRT/bert-99.9/Offline"> 5189.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0004 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9V84 96-Core Processor
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 80
Processors per node: 1
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Azure/systems/NC_H100_v5_TRT.json"> NC80adis_H100_v5 (2x H100-NVL-94GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Azure </td>
        <td class="col-accelerator headcol"> NVIDIA H100 NVL x 2 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Azure/results/NC_H100_v5_TRT/llama2-70b-99.9/Offline"> 11.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Azure/results/NC_H100_v5_TRT/llama2-70b-99.9/Offline"> 3900.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Azure/results/NC_H100_v5_TRT/gptj-99/Offline"> 42.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Azure/results/NC_H100_v5_TRT/gptj-99/Offline"> 43.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Azure/results/NC_H100_v5_TRT/3d-unet-99/Offline"> 10.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Azure/results/NC_H100_v5_TRT/3d-unet-99.9/Offline"> 10.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0005 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
Software: QUALCOMM Cloud AI SDK v1.10.0
Cores per processor: 16
Processors per node: 2
Nodes: 1
Notes: Automated by MLCommons CM v2.0.0. 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/systems/Cirrascale_4xQAIC_AI_100-kilt-qaic-glow-vdefault-default_config.json"> Cirrascale_QUAD_AI100 (4x QAIC Pro) </a> </td>
        <td class="col-submitter headcol"> CTuning </td>
        <td class="col-accelerator headcol"> Qualcomm Cloud AI 100 Pro x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8,fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/results/Cirrascale_4xQAIC_AI_100-kilt-qaic-glow-vdefault-default_config/bert-99/offline"> 2551.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8,fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/results/Cirrascale_4xQAIC_AI_100-kilt-qaic-glow-vdefault-default_config/bert-99/offline"> 3150.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0006 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) w7-2495X
Software: Intel inference implementation with CM API, Pytorch v1.12
Cores per processor: 24
Processors per node: 1
Nodes: 1
Notes: Automated by MLCommons CM v2.0.0. 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/systems/GATEOverflow_Intel_Saphhire_Rapids-intel-cpu-pytorch-vdefault-default_config.json"> GATEOverflow Intel Saphhire Rapids </a> </td>
        <td class="col-submitter headcol"> CTuning </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/results/GATEOverflow_Intel_Saphhire_Rapids-intel-cpu-pytorch-vdefault-default_config/bert-99/offline"> 279.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/results/GATEOverflow_Intel_Saphhire_Rapids-intel-cpu-pytorch-vdefault-default_config/bert-99/offline"> 370.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0007 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz
Software: QUALCOMM Cloud AI Platform SDK v1.12.2, Apps SDK v1.14.2
Cores per processor: 24
Processors per node: 2
Nodes: 1
Notes: Automated by MLCommons CM v1.6.2. 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/systems/aws_dl2q.24xlarge-kilt-qaic-glow-vdefault-default_config.json"> AWS_EC2_DL2Q (8x QAIC Standard) </a> </td>
        <td class="col-submitter headcol"> CTuning </td>
        <td class="col-accelerator headcol"> Qualcomm Cloud AI 100 Standard x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/results/aws_dl2q.24xlarge-kilt-qaic-glow-vdefault-default_config/retinanet/offline"> 2199.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/results/aws_dl2q.24xlarge-kilt-qaic-glow-vdefault-default_config/retinanet/offline"> 2493.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/results/aws_dl2q.24xlarge-kilt-qaic-glow-vdefault-default_config/resnet50/offline"> 148979.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/CTuning/results/aws_dl2q.24xlarge-kilt-qaic-glow-vdefault-default_config/resnet50/offline"> 153133.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0022 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8592+
Software: PyTorch
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/systems/1-node-2S-C240M7-EMR-PyTorch-INT4.json"> 1-node-2S-C240M7-EMR-PyTorch-INT4 </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/results/1-node-2S-C240M7-EMR-PyTorch-INT4/gptj-99/Offline"> 1.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/results/1-node-2S-C240M7-EMR-PyTorch-INT4/gptj-99/Offline"> 2.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0023 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8592+
Software: PyTorch
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/systems/1-node-2S-C240M7-EMR-PyTorch-INT8.json"> 1-node-2S-C240M7-EMR-PyTorch-INT8 </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/results/1-node-2S-C240M7-EMR-PyTorch-INT8/bert-99/Offline"> 1318.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/results/1-node-2S-C240M7-EMR-PyTorch-INT8/bert-99/Offline"> 1693.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/results/1-node-2S-C240M7-EMR-PyTorch-INT8/retinanet/Offline"> 303.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/results/1-node-2S-C240M7-EMR-PyTorch-INT8/retinanet/Offline"> 389.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/results/1-node-2S-C240M7-EMR-PyTorch-INT8/resnet50/Offline"> 19807.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/results/1-node-2S-C240M7-EMR-PyTorch-INT8/resnet50/Offline"> 25704.9 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0024 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8592V
Software: PyTorch
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Cisco/systems/1-node-2S-C240M7-EMR-PyTorch-MIX.json"> 1-node-2S-C240M7-EMR-PyTorch-MIX </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0025 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz
Software: TensorRT 9.0.0, CUDA 12.2
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/R750xa_A100_PCIe_80GBx4_TRT.json"> Dell PowerEdge R750xa (4x A100-PCIe-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA A100-PCIe-80GB x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/bert-99/Offline"> 11854.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/bert-99/Offline"> 12669.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/bert-99.9/Offline"> 5797.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/bert-99.9/Offline"> 6491.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/retinanet/Offline"> 2831.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/retinanet/Offline"> 2947.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/resnet50/Offline"> 144586.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/resnet50/Offline"> 157826.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/3d-unet-99/Offline"> 14.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R750xa_A100_PCIe_80GBx4_TRT/3d-unet-99.9/Offline"> 14.2 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0026 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8592+
Software: PyTorch
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: INT4 for GPT-J, Mixed for RNN-T and INT8 for all other models
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/R760_CPU.json"> Dell PowerEdge R760 </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/gptj-99/Offline"> 1.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/gptj-99/Offline"> 2.4 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/bert-99/Offline"> 1318.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/bert-99/Offline"> 1701.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/dlrm-v2-99.9/Offline"> 8993.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/dlrm-v2-99.9/Offline"> 9239.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/retinanet/Offline"> 299.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/retinanet/Offline"> 382.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/resnet50/Offline"> 19807.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/resnet50/Offline"> 25016.0 </a> </td>

                <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_CPU/3d-unet-99.9/Offline"> 1.9 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0027 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8580
Software: TensorRT 9.3.0, CUDA 12.3
Cores per processor: 120
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/R760_L40Sx2_TRT.json"> Dell PowerEdge R760 (2x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/bert-99/Offline"> 6696.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/bert-99/Offline"> 6603.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/bert-99.9/Offline"> 3351.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/bert-99.9/Offline"> 3507.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/retinanet/Offline"> 1523.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/retinanet/Offline"> 1642.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/resnet50/Offline"> 88074.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/resnet50/Offline"> 90387.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/3d-unet-99/Offline"> 7.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760_L40Sx2_TRT/3d-unet-99.9/Offline"> 7.8 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0028 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/R760xa_L40Sx4_TRT.json"> Dell PowerEdge R760xa (4x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/gptj-99/Offline"> 51.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/gptj-99/Offline"> 50.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/gptj-99.9/Offline"> 51.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/gptj-99.9/Offline"> 50.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/bert-99/Offline"> 13695.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/bert-99/Offline"> 13538.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/bert-99.9/Offline"> 6916.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/bert-99.9/Offline"> 7308.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/stable-diffusion-xl/Offline"> 2.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/stable-diffusion-xl/Offline"> 2.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/retinanet/Offline"> 3062.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/retinanet/Offline"> 3196.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/resnet50/Offline"> 179615.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/resnet50/Offline"> 175746.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/3d-unet-99/Offline"> 15.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R760xa_L40Sx4_TRT/3d-unet-99.9/Offline"> 15.6 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0029 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9354 32-Core Processor
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/R7615_L40Sx2_TRT.json"> Dell PowerEdge R7615 (2x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/gptj-99/Offline"> 25.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/gptj-99/Offline"> 25.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/gptj-99.9/Offline"> 25.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/gptj-99.9/Offline"> 25.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/bert-99/Offline"> 6666.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/bert-99/Offline"> 6576.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/bert-99.9/Offline"> 3326.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/bert-99.9/Offline"> 3514.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/stable-diffusion-xl/Offline"> 1.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/stable-diffusion-xl/Offline"> 1.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/retinanet/Offline"> 1508.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/retinanet/Offline"> 1618.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/resnet50/Offline"> 90571.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/resnet50/Offline"> 88893.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/3d-unet-99/Offline"> 7.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/R7615_L40Sx2_TRT/3d-unet-99.9/Offline"> 7.8 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0030 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8468
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 96
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/XE8640_H100_SXM_80GBx4_TRT.json"> Dell PowerEdge XE8640 (4x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 4 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/llama2-70b-99.9/Offline"> 31.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/llama2-70b-99.9/Offline"> 9896.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/gptj-99/Offline"> 115.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/gptj-99/Offline"> 117.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/gptj-99.9/Offline"> 115.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/gptj-99.9/Offline"> 117.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/bert-99/Offline"> 28553.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/bert-99/Offline"> 35790.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/bert-99.9/Offline"> 25385.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/bert-99.9/Offline"> 31462.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/stable-diffusion-xl/Offline"> 6.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/stable-diffusion-xl/Offline"> 6.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/dlrm-v2-99/Offline"> 258743.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/dlrm-v2-99/Offline"> 278414.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/dlrm-v2-99.9/Offline"> 169980.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/dlrm-v2-99.9/Offline"> 174149.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/retinanet/Offline"> 6759.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/retinanet/Offline"> 7081.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/resnet50/Offline"> 310282.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/resnet50/Offline"> 353746.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/3d-unet-99/Offline"> 25.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/3d-unet-99.9/Offline"> 25.9 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0031 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/XE9640_H100_SXM_80GBX4_TRT.json"> Dell PowerEdge XE9640 (4x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 4 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/llama2-70b-99.9/Offline"> 30.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/llama2-70b-99.9/Offline"> 9248.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/gptj-99/Offline"> 112.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/gptj-99/Offline"> 117.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/gptj-99.9/Offline"> 112.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/gptj-99.9/Offline"> 117.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/bert-99/Offline"> 28338.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/bert-99/Offline"> 36146.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/bert-99.9/Offline"> 24846.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/bert-99.9/Offline"> 31417.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/stable-diffusion-xl/Offline"> 6.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/stable-diffusion-xl/Offline"> 6.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/retinanet/Offline"> 6733.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/retinanet/Offline"> 7025.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/resnet50/Offline"> 305775.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/resnet50/Offline"> 352757.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/3d-unet-99/Offline"> 25.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9640_H100_SXM_80GBX4_TRT/3d-unet-99.9/Offline"> 25.8 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0032 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/XE9680_H100_SXM_80GBx8_TRT.json"> Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Offline"> 71.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Offline"> 21884.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99/Offline"> 233.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99/Offline"> 231.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99.9/Offline"> 233.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99.9/Offline"> 231.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/bert-99/Offline"> 57083.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/bert-99/Offline"> 70189.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/bert-99.9/Offline"> 51186.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/bert-99.9/Offline"> 62310.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Offline"> 13.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Offline"> 12.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/dlrm-v2-99/Offline"> 512410.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/dlrm-v2-99/Offline"> 554313.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/dlrm-v2-99.9/Offline"> 331018.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/dlrm-v2-99.9/Offline"> 347289.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/retinanet/Offline"> 13615.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/retinanet/Offline"> 14160.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/resnet50/Offline"> 630172.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/resnet50/Offline"> 705088.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/3d-unet-99/Offline"> 52.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/3d-unet-99.9/Offline"> 52.0 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0033 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8470
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 52
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/XE9680_H100_SXM_80GBx8_TRT_MaxQ.json"> Dell PowerEdge XE9680 (8x H100-SXM-80GB, MaxQ, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/llama2-70b-99.9/Offline"> 49.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/llama2-70b-99.9/Offline"> 17098.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/gptj-99/Offline"> 149.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/gptj-99/Offline"> 174.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/gptj-99.9/Offline"> 149.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/gptj-99.9/Offline"> 174.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/stable-diffusion-xl/Offline"> 8.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/stable-diffusion-xl/Offline"> 9.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/resnet50/Offline"> 400031.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/resnet50/Offline"> 456575.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/3d-unet-99/Offline"> 37.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT_MaxQ/3d-unet-99.9/Offline"> 37.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0034 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon Platinum 8480
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: Broadcom VM Specifications 32vCPU out of 224 and memory of 128 GB out of 1TB
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT.json"> Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT,VMware ESXi 8.0.2)  </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> Virtualized NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT/bert-99/Offline"> 55982.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT/bert-99/Offline"> 69897.7 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Offline"> 13.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Offline"> 13.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT/retinanet/Offline"> 12876.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT/retinanet/Offline"> 14091.2 </a> </td>

                <td></td>

                    <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT/3d-unet-99/Offline"> 51.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/XE9680_IntelXeon8480_NVIDIA_H100_SXM_80GBx8_TRT/3d-unet-99.9/Offline"> 51.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0035 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon Platinum 8580
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 60
Processors per node: 2
Nodes: 1
Notes: Broadcom VM Specifications 32vCPU out of 120 and memory of 128 GB out of 1.5TB
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/vR760_L40S_48Cx2_TRT.json"> Dell PowerEdge R760 (2x L40S, TensorRT, VMware ESXi 8.0.2) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> Virtualized NVIDIA L40S x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/vR760_L40S_48Cx2_TRT/bert-99/Offline"> 5996.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/vR760_L40S_48Cx2_TRT/bert-99/Offline"> 6082.8 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/vR760_L40S_48Cx2_TRT/stable-diffusion-xl/Offline"> 1.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/vR760_L40S_48Cx2_TRT/stable-diffusion-xl/Offline"> 1.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/vR760_L40S_48Cx2_TRT/retinanet/Offline"> 1473.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/vR760_L40S_48Cx2_TRT/retinanet/Offline"> 1584.8 </a> </td>

                <td></td>

                    <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/vR760_L40S_48Cx2_TRT/3d-unet-99/Offline"> 7.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/vR760_L40S_48Cx2_TRT/3d-unet-99.9/Offline"> 7.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0039 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8452Y
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 36
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/systems/CDI_L40Sx16_TRT.json"> PRIMERGY CDI (16x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Fujitsu </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 16 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/CDI_L40Sx16_TRT/stable-diffusion-xl/Offline"> 10.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/CDI_L40Sx16_TRT/stable-diffusion-xl/Offline"> 10.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0040 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8452Y
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 36
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/systems/CDI_L40Sx8_MaxP_TRT.json"> PRIMERGY CDI (8x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Fujitsu </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/CDI_L40Sx8_MaxP_TRT/gptj-99/Offline"> 95.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/CDI_L40Sx8_MaxP_TRT/gptj-99/Offline"> 95.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/CDI_L40Sx8_MaxP_TRT/gptj-99.9/Offline"> 95.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/CDI_L40Sx8_MaxP_TRT/gptj-99.9/Offline"> 95.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0041 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8452Y
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 36
Processors per node: 2
Nodes: 1
Notes: GPU Power Limit: 280W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/systems/CDI_L40Sx8_TRT.json"> PRIMERGY CDI (8x L40S, MaxQ, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Fujitsu </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/CDI_L40Sx8_TRT/stable-diffusion-xl/Offline"> 4.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/CDI_L40Sx8_TRT/stable-diffusion-xl/Offline"> 4.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0042 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8468
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/systems/GX2560M7_H100_SXM_80GBx4_TRT.json"> GX2560M7_H100_SXM_80GBx4 (4x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Fujitsu </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 4 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/llama2-70b-99.9/Offline"> 27.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/llama2-70b-99.9/Offline"> 8436.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/gptj-99/Offline"> 85.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/gptj-99/Offline"> 112.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/gptj-99.9/Offline"> 85.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/gptj-99.9/Offline"> 112.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/stable-diffusion-xl/Offline"> 6.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/stable-diffusion-xl/Offline"> 6.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/resnet50/Offline"> 295002.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/resnet50/Offline"> 352326.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0043 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8592+
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 600GB/s
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/systems/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT.json"> GIGABYTE G593-SD1 </a> </td>
        <td class="col-submitter headcol"> GigaComputing </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 72.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 22289.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/gptj-99/Offline"> 229.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/gptj-99/Offline"> 240.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/gptj-99.9/Offline"> 229.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/gptj-99.9/Offline"> 240.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/bert-99/Offline"> 57292.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/bert-99/Offline"> 70438.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/bert-99.9/Offline"> 51186.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/bert-99.9/Offline"> 61805.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/stable-diffusion-xl/Offline"> 13.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/stable-diffusion-xl/Offline"> 13.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99/Offline"> 327017.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99/Offline"> 558571.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99.9/Offline"> 327017.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99.9/Offline"> 347454.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/retinanet/Offline"> 12995.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/retinanet/Offline"> 14259.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/resnet50/Offline"> 630172.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/resnet50/Offline"> 704559.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/3d-unet-99/Offline"> 51.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/GigaComputing/results/GIGABYTE-G593-SD1_DGX-H100_H100-SXM-80GBx8_TRT/3d-unet-99.9/Offline"> 51.8 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0044 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8481C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 208
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/systems/H100-SXM-80GBx8_TRT.json"> NVIDIA DGX H100 (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Google </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 69.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 20992.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/gptj-99/Offline"> 229.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/gptj-99/Offline"> 235.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/gptj-99.9/Offline"> 229.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/gptj-99.9/Offline"> 235.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/bert-99/Offline"> 55982.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/bert-99/Offline"> 69671.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/bert-99.9/Offline"> 49586.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/bert-99.9/Offline"> 61292.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/stable-diffusion-xl/Offline"> 13.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/stable-diffusion-xl/Offline"> 12.9 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/dlrm-v2-99.9/Offline"> 312469.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/dlrm-v2-99.9/Offline"> 341111.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/retinanet/Offline"> 12876.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/retinanet/Offline"> 14031.2 </a> </td>

                <td></td>

                    <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/3d-unet-99/Offline"> 51.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/H100-SXM-80GBx8_TRT/3d-unet-99.9/Offline"> 51.2 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0045 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 7B13
Software: Saxml
Cores per processor: 56
Processors per node: 1
Nodes: 1
Notes: Powered by the KRAI X technology
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/systems/tpu_v5e_x4.json"> tpu-v5e-4 </a> </td>
        <td class="col-submitter headcol"> Google </td>
        <td class="col-accelerator headcol"> TPU v5e x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/tpu_v5e_x4/gptj-99/offline"> 7.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Google/results/tpu_v5e_x4/gptj-99/offline"> 10.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0046 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8468
Software: TensorRT 9.3.0, CUDA 12.3
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/systems/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT.json"> HPE Cray XD670 (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/llama2-70b-99.9/Offline"> 64.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/llama2-70b-99.9/Offline"> 20973.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/bert-99/Offline"> 57183.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/bert-99/Offline"> 70758.8 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/stable-diffusion-xl/Offline"> 13.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/stable-diffusion-xl/Offline"> 13.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/retinanet/Offline"> 13675.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/retinanet/Offline"> 14291.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/resnet50/Offline"> 621165.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/resnet50/Offline"> 705059.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/3d-unet-99/Offline"> 51.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/3d-unet-99.9/Offline"> 51.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0047 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8468
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/systems/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT.json"> HPE ProLiant DL380a Gen11 (4x H100-PCIe-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H100-PCIe-80GB x 4 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/llama2-70b-99.9/Offline"> 19.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/llama2-70b-99.9/Offline"> 6335.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/retinanet/Offline"> 4299.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/retinanet/Offline"> 4367.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/resnet50/Offline"> 203997.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/resnet50/Offline"> 215959.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/3d-unet-99/Offline"> 18.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/3d-unet-99.9/Offline"> 18.0 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0048 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8580
Software: TensorRT 9.0.0, CUDA 12.2
Cores per processor: 60
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/systems/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT.json"> HPE ProLiant DL380a Gen11 (4x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/bert-99/Offline"> 11594.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/bert-99/Offline"> 12928.7 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/stable-diffusion-xl/Offline"> 2.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/stable-diffusion-xl/Offline"> 2.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/retinanet/Offline"> 2801.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/retinanet/Offline"> 3148.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/resnet50/Offline"> 160379.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/resnet50/Offline"> 156816.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/3d-unet-99/Offline"> 15.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/3d-unet-99.9/Offline"> 15.5 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0049 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8592+
Software: PyTorch
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D54Q-2U. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/systems/1-node-2S-EMR-ITREX.json"> 1-node-2S-EMR-ITREX </a> </td>
        <td class="col-submitter headcol"> Intel </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int4" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-ITREX/gptj-99/Offline"> 1.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int4" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-ITREX/gptj-99/Offline"> 3.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0050 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8592+
Software: PyTorch
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D54Q-2U. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/systems/1-node-2S-EMR-PyTorch.json"> 1-node-2S-EMR-PyTorch </a> </td>
        <td class="col-submitter headcol"> Intel </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/bert-99/Offline"> 1318.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/bert-99/Offline"> 1668.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/dlrm-v2-99.9/Offline"> 8993.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/dlrm-v2-99.9/Offline"> 9111.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/retinanet/Offline"> 274.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/retinanet/Offline"> 371.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/resnet50/Offline"> 19807.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/resnet50/Offline"> 25289.6 </a> </td>

                <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel/results/1-node-2S-EMR-PyTorch/3d-unet-99.9/Offline"> 2.0 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0052 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8380
Software: PyTorch 2.1.1
Cores per processor: 40
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel-HabanaLabs/systems/HLS-Gaudi2-PT.json"> HLS-Gaudi2-PT </a> </td>
        <td class="col-submitter headcol"> Intel-HabanaLabs </td>
        <td class="col-accelerator headcol"> Intel® Gaudi® 2 AI Accelerator x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp8-E4M3" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel-HabanaLabs/results/HLS-Gaudi2-PT/llama2-70b-99.9/Offline"> 21.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp8-E4M3" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel-HabanaLabs/results/HLS-Gaudi2-PT/llama2-70b-99.9/Offline"> 8034.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: mixed precision fp8-E4M3,bfloat16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel-HabanaLabs/results/HLS-Gaudi2-PT/stable-diffusion-xl/Offline"> 6.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: mixed precision fp8-E4M3,bfloat16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Intel-HabanaLabs/results/HLS-Gaudi2-PT/stable-diffusion-xl/Offline"> 6.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0053 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 7713 64-Core Processor
Software: QUALCOMM Cloud AI SDK v1.14.2
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: Powered by the KRAI X and KILT technologies
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Krai/systems/g292_q16_pro_pp.json"> GIGABYTE G292-Z43 (16x QAIC100 Pro) </a> </td>
        <td class="col-submitter headcol"> Krai </td>
        <td class="col-accelerator headcol"> QUALCOMM Cloud AI 100 Pro x 16 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Krai/results/g292_q16_pro_pp/stable-diffusion-xl/offline"> 0.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Krai/results/g292_q16_pro_pp/stable-diffusion-xl/offline"> 1.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0054 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 8124P 16-Core Processor
Software: TensorRT 9.3.0, CUDA 12.3
Cores per processor: 32
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/systems/SE455_L40x2_TRT.json"> Lenovo SE455 (2x NVIDIA L40, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol"> NVIDIA L40 x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/SE455_L40x2_TRT/bert-99.9/Offline"> 1699.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/SE455_L40x2_TRT/bert-99.9/Offline"> 1699.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/SE455_L40x2_TRT/retinanet/Offline"> 949.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/SE455_L40x2_TRT/retinanet/Offline"> 888.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/SE455_L40x2_TRT/resnet50/Offline"> 59982.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/SE455_L40x2_TRT/resnet50/Offline"> 47155.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/SE455_L40x2_TRT/3d-unet-99/Offline"> 6.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/SE455_L40x2_TRT/3d-unet-99.9/Offline"> 6.0 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0057 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/DGX-H100_H100-SXM-80GBx1_TRT.json"> NVIDIA DGX H100 (1x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 1 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/gptj-99/Offline"> 29.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/gptj-99/Offline"> 31.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/gptj-99.9/Offline"> 29.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/gptj-99.9/Offline"> 31.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/bert-99/Offline"> 6995.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/bert-99/Offline"> 9081.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/bert-99.9/Offline"> 6095.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/bert-99.9/Offline"> 7939.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/stable-diffusion-xl/Offline"> 1.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/stable-diffusion-xl/Offline"> 1.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/dlrm-v2-99/Offline"> 65982.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/dlrm-v2-99/Offline"> 70346.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/dlrm-v2-99.9/Offline"> 42985.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/dlrm-v2-99.9/Offline"> 44022.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/retinanet/Offline"> 1618.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/retinanet/Offline"> 1728.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/resnet50/Offline"> 72976.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/resnet50/Offline"> 88714.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/3d-unet-99/Offline"> 6.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT/3d-unet-99.9/Offline"> 6.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0058 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/DGX-H100_H100-SXM-80GBx2_TRT.json"> NVIDIA DGX H100 (2x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 2 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx2_TRT/llama2-70b-99.9/Offline"> 18.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx2_TRT/llama2-70b-99.9/Offline"> 5539.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0059 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/DGX-H100_H100-SXM-80GBx8_TRT.json"> NVIDIA DGX H100 (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 70.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 21805.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/gptj-99/Offline"> 229.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/gptj-99/Offline"> 239.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/gptj-99.9/Offline"> 229.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/gptj-99.9/Offline"> 239.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/bert-99/Offline"> 55982.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/bert-99/Offline"> 70116.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/bert-99.9/Offline"> 49586.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/bert-99.9/Offline"> 62182.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/stable-diffusion-xl/Offline"> 13.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/stable-diffusion-xl/Offline"> 13.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99/Offline"> 500098.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99/Offline"> 568422.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99.9/Offline"> 330016.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99.9/Offline"> 354151.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/retinanet/Offline"> 12876.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/retinanet/Offline"> 14194.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/resnet50/Offline"> 584147.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/resnet50/Offline"> 703770.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/3d-unet-99/Offline"> 51.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/3d-unet-99.9/Offline"> 51.5 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0060 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ.json"> NVIDIA DGX H100 (8x H100-SXM-80GB, MaxQ, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/llama2-70b-99.9/Offline"> 53.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/llama2-70b-99.9/Offline"> 17561.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/gptj-99/Offline"> 149.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/gptj-99/Offline"> 178.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/gptj-99.9/Offline"> 149.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/gptj-99.9/Offline"> 178.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/bert-99/Offline"> 42386.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/bert-99/Offline"> 53726.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/bert-99.9/Offline"> 39186.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/bert-99.9/Offline"> 50998.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/stable-diffusion-xl/Offline"> 8.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/stable-diffusion-xl/Offline"> 9.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/dlrm-v2-99/Offline"> 400031.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/dlrm-v2-99/Offline"> 458408.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/dlrm-v2-99.9/Offline"> 255995.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/dlrm-v2-99.9/Offline"> 283714.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/retinanet/Offline"> 8794.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/retinanet/Offline"> 10105.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/resnet50/Offline"> 400031.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/resnet50/Offline"> 473737.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/3d-unet-99/Offline"> 38.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_MaxQ/3d-unet-99.9/Offline"> 38.2 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0061 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 72
Processors per node: 1
Nodes: 1
Notes: NVIDIA MGX Reference Platform;
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT.json"> NVIDIA GH200-GraceHopper-Superchip (1x GH200-96GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA GH200-GraceHopper-Superchip x 1 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/gptj-99/Offline"> 31.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/gptj-99/Offline"> 31.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/gptj-99.9/Offline"> 31.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/gptj-99.9/Offline"> 31.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/stable-diffusion-xl/Offline"> 1.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/stable-diffusion-xl/Offline"> 1.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/dlrm-v2-99/Offline"> 78702.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/dlrm-v2-99/Offline"> 80118.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/dlrm-v2-99.9/Offline"> 48788.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/dlrm-v2-99.9/Offline"> 49651.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0062 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/H100-NVL-94GBx8_TRT.json"> SYS-521GE-TNRT (8x H100-NVL-94GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H100 NVL x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H100-NVL-94GBx8_TRT/llama2-70b-99.9/Offline"> 48.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H100-NVL-94GBx8_TRT/llama2-70b-99.9/Offline"> 15085.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0063 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 7742 64-Core Processor
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/L40Sx8_TRT.json"> ASROCKRACK 4U8G-ROME2/4E (8x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/gptj-99/Offline"> 95.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/gptj-99/Offline"> 94.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/gptj-99.9/Offline"> 95.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/gptj-99.9/Offline"> 94.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/stable-diffusion-xl/Offline"> 4.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/stable-diffusion-xl/Offline"> 5.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/dlrm-v2-99/Offline"> 179985.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/dlrm-v2-99/Offline"> 175953.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/dlrm-v2-99.9/Offline"> 84976.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/L40Sx8_TRT/dlrm-v2-99.9/Offline"> 85117.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0065 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 9.3.0.1, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/systems/OCI-H100_H100-SXM-80GBx8_TRT.json"> BM.GPU.H100.8 </a> </td>
        <td class="col-submitter headcol"> Oracle </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 69.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Offline"> 21032.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/gptj-99/Offline"> 229.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/gptj-99/Offline"> 237.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/gptj-99.9/Offline"> 229.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/gptj-99.9/Offline"> 235.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/bert-99/Offline"> 55982.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/bert-99/Offline"> 69820.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/bert-99.9/Offline"> 49586.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/bert-99.9/Offline"> 61817.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/stable-diffusion-xl/Offline"> 13.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/stable-diffusion-xl/Offline"> 13.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99/Offline"> 500098.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99/Offline"> 557592.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99.9/Offline"> 315013.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/dlrm-v2-99.9/Offline"> 347177.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/retinanet/Offline"> 12876.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/retinanet/Offline"> 13997.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/resnet50/Offline"> 584147.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/resnet50/Offline"> 699409.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/3d-unet-99/Offline"> 51.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Oracle/results/OCI-H100_H100-SXM-80GBx8_TRT/3d-unet-99.9/Offline"> 51.5 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0067 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8592+
Software: PyTorch
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D54X-1U
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/systems/1-node-2S-EMR-PyTorch.json"> 1-node-2S-EMR-PyTorch-INT8 </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/gptj-99/Offline"> 1.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/gptj-99/Offline"> 2.3 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/bert-99/Offline"> 1288.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/bert-99/Offline"> 1660.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/dlrm-v2-99.9/Offline"> 8193.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/dlrm-v2-99.9/Offline"> 9245.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/retinanet/Offline"> 279.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/retinanet/Offline"> 379.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/resnet50/Offline"> 23198.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/resnet50/Offline"> 25173.0 </a> </td>

                <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/1-node-2S-EMR-PyTorch/3d-unet-99.9/Offline"> 2.0 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0068 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8470
Software: TensorRT 9.3.0, CUDA 12.3
Cores per processor: 52
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D54U-3U
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/systems/D54U_3U_H100_PCIe_80GBx4_TRT.json"> D54U_3U_H100_PCIe_80GBx4_TRT </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> NVIDIA H100-PCIe-80GB x 4 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99.9/Offline"> 13.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99.9/Offline"> 4407.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99/Offline"> 79.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99/Offline"> 82.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99.9/Offline"> 79.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99.9/Offline"> 82.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/bert-99/Offline"> 17997.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/bert-99/Offline"> 23347.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/bert-99.9/Offline"> 15996.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/bert-99.9/Offline"> 19441.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/stable-diffusion-xl/Offline"> 4.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/stable-diffusion-xl/Offline"> 4.4 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99.9/Offline"> 97970.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99.9/Offline"> 102225.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/retinanet/Offline"> 4001.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/retinanet/Offline"> 4534.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/resnet50/Offline"> 187992.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/resnet50/Offline"> 222035.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/3d-unet-99/Offline"> 18.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/3d-unet-99.9/Offline"> 18.4 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0069 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8470
Software: TensorRT 9.3.0, CUDA 12.3
Cores per processor: 52
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D54U-3U
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/systems/D54U_3U_L40S_PCIe_48GBx4_TRT.json"> D54U_3U_L40S_PCIe_48GBx4_TRT </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/gptj-99/Offline"> 45.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/gptj-99/Offline"> 50.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/gptj-99.9/Offline"> 45.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/gptj-99.9/Offline"> 50.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/bert-99/Offline"> 11995.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/bert-99/Offline"> 13226.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/bert-99.9/Offline"> 5397.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/bert-99.9/Offline"> 6845.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/stable-diffusion-xl/Offline"> 2.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/stable-diffusion-xl/Offline"> 2.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/dlrm-v2-99/Offline"> 84376.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/dlrm-v2-99/Offline"> 97314.9 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/dlrm-v2-99.9/Offline"> 47988.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/dlrm-v2-99.9/Offline"> 51318.5 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/retinanet/Offline"> 3002.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/retinanet/Offline"> 3181.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/resnet50/Offline"> 149980.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/resnet50/Offline"> 176074.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/3d-unet-99/Offline"> 15.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/D54U_3U_L40S_PCIe_48GBx4_TRT/3d-unet-99.9/Offline"> 15.5 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0070 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 72
Processors per node: 1
Nodes: 1
Notes: QuantaGrid S74G-2U
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/systems/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT.json"> GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> NVIDIA GH200-GraceHopper-Superchip x 1 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/gptj-99/Offline"> 31.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/gptj-99/Offline"> 32.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/stable-diffusion-xl/Offline"> 1.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/stable-diffusion-xl/Offline"> 1.8 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/dlrm-v2-99.9/Offline"> 47688.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Quanta_Cloud_Technology/results/GH200-GraceHopper-Superchip_GH200-96GB_aarch64x1_TRT/dlrm-v2-99.9/Offline"> 48403.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0072 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8592+
Software: PyTorch
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/systems/1-node-2S-EMR-PyTorch-INT8.json"> 1-node-2S-EMR-PyTorch-INT8 </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/1-node-2S-EMR-PyTorch-INT8/resnet50/Offline"> 19807.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/1-node-2S-EMR-PyTorch-INT8/resnet50/Offline"> 24146.0 </a> </td>

                <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/1-node-2S-EMR-PyTorch-INT8/3d-unet-99.9/Offline"> 2.0 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0073 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8462Y+
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 32
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/systems/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT.json"> SYS-521GE-TNRT (8xL40S-PCIe-48GB) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/bert-99/Offline"> 11995.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/bert-99/Offline"> 26430.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/bert-99.9/Offline"> 10994.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/bert-99.9/Offline"> 14002.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/retinanet/Offline"> 1999.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/retinanet/Offline"> 6379.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/resnet50/Offline"> 127980.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/resnet50/Offline"> 345124.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/3d-unet-99/Offline"> 31.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS-521GE-TNRT_L40S_PCIe_48GBx8_TRT/3d-unet-99.9/Offline"> 31.7 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0074 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8568Y+
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/systems/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT.json"> SYS-821GE-TNHR (8x H100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Offline"> 73.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Offline"> 22202.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/gptj-99/Offline"> 239.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/gptj-99/Offline"> 243.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/gptj-99.9/Offline"> 239.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/gptj-99.9/Offline"> 243.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/bert-99/Offline"> 56775.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/bert-99/Offline"> 70342.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/bert-99.9/Offline"> 50659.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/bert-99.9/Offline"> 62226.4 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Offline"> 13.6 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/stable-diffusion-xl/Offline"> 13.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/dlrm-v2-99/Offline"> 516110.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/dlrm-v2-99/Offline"> 560530.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/dlrm-v2-99.9/Offline"> 333218.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/dlrm-v2-99.9/Offline"> 350225.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/retinanet/Offline"> 12940.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/retinanet/Offline"> 13756.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/resnet50/Offline"> 595963.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/resnet50/Offline"> 705887.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/3d-unet-99/Offline"> 52.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/3d-unet-99.9/Offline"> 52.0 </a> </td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0075 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) GOLD 6538Y+
Software: PyTorch
Cores per processor: 32
Processors per node: 1
Nodes: 1
Notes: Wiwynn ES200G2. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/systems/1-node-1S-EMR-PyTorch.json"> Wiwynn ES200G2 (1-node-1S-EMR-PyTorch) </a> </td>
        <td class="col-submitter headcol"> Wiwynn </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/gptj-99/Offline"> 0.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/gptj-99/Offline"> 0.6 </a> </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/bert-99/Offline"> 328.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/bert-99/Offline"> 467.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/retinanet/Offline"> 61.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/retinanet/Offline"> 109.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/resnet50/Offline"> 4951.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/resnet50/Offline"> 7402.9 </a> </td>

                <td></td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Wiwynn/results/1-node-1S-EMR-PyTorch/3d-unet-99.9/Offline"> 0.7 </a> </td>

        </tr>
        </table></div>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<hr>

        <h2 id="results_heading_preview" class="results_table_heading">Datacenter Category: Preview submissions in Closed division</h2>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<div id="results_table_preview" class="resultstable_wrapper"> <table class="resultstable tablesorter tableclosed tabledatacenter" id="results_preview"><thead> <tr>
        <th id="col-id" class="headcol col-id">ID</th>
        <th id="col-system" class="headcol col-system">System</th>
        <th id="col-submitter" class="headcol col-submitter">Submitter</th>
        <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
        <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
        <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
        <th id="col-gptj-99" colspan="2">GPTJ-99</th>
        <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
        <th id="col-bert-99" colspan="2">Bert-99</th>
        <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
        <th id="col-dlrm-v2-99" colspan="2">Stable Diffusion</th>
        <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
        <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
        <th id="col-retinanet" colspan="2">Retinanet</th>
        <th id="col-resnet50" colspan="2">ResNet50</th>
        <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
        <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
        </tr>
    <tr>
    <th class="headcol col-id"></th>
    <th class="headcol col-system"></th>
    <th class="headcol col-submitter"></th>
    <th class="headcol col-accelerator"></th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Offline</th>
    </tr></thead><tfoot> <tr>
        <th id="col-id" class="headcol col-id">ID</th>
        <th id="col-system" class="headcol col-system">System</th>
        <th id="col-submitter" class="headcol col-submitter">Submitter</th>
        <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
        <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
        <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
        <th id="col-gptj-99" colspan="2">GPTJ-99</th>
        <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
        <th id="col-bert-99" colspan="2">Bert-99</th>
        <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
        <th id="col-dlrm-v2-99" colspan="2">Stable Diffusion</th>
        <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
        <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
        <th id="col-retinanet" colspan="2">Retinanet</th>
        <th id="col-resnet50" colspan="2">ResNet50</th>
        <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
        <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
        </tr>
    <tr>
    <th class="headcol col-id"></th>
    <th class="headcol col-system"></th>
    <th class="headcol col-submitter"></th>
    <th class="headcol col-accelerator"></th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Server</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Offline</th>
    <th class="col-scenario">Offline</th>
    </tr></tfoot>
        <tr>
        <td class="col-id headcol"> 4.0-0077 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Gold 6448Y
Software: QUALCOMM Cloud AI SDK v1.12.2
Cores per processor: 32
Processors per node: 2
Nodes: 1
Notes: Powered by the KRAI X and KILT technologies
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/systems/r760_q4_ultra.json"> Dell PowerEdge R760xa (4x QAIC100 Ultra) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> QUALCOMM Cloud AI 100 Ultra x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/r760_q4_ultra/bert-99/offline"> 6246.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/r760_q4_ultra/bert-99/offline"> 7262.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/r760_q4_ultra/bert-99.9/offline"> 2099.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/r760_q4_ultra/bert-99.9/offline"> 4147.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/r760_q4_ultra/retinanet/offline"> 3126.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/r760_q4_ultra/retinanet/offline"> 3536.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/r760_q4_ultra/resnet50/offline"> 215003.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Dell/results/r760_q4_ultra/resnet50/offline"> 218095.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0078 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Gold 6448H
Software: QUALCOMM Cloud AI SDK v1.12.2
Cores per processor: 32
Processors per node: 2
Nodes: 1
Notes: Powered by the KRAI X and KILT technologies
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/systems/dl380_q8_ultra.json"> HPE ProLiant DL380a Gen11 (8x Qualcomm QAIC100 Ultra) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> QUALCOMM Cloud AI 100 Ultra x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/dl380_q8_ultra/bert-99/offline"> 12871.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/dl380_q8_ultra/bert-99/offline"> 13132.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/dl380_q8_ultra/resnet50/offline"> 370025.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/HPE/results/dl380_q8_ultra/resnet50/offline"> 391186.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0079 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8360Y CPU @ 2.40GHz
Software: QUALCOMM Cloud AI SDK v1.14.2
Cores per processor: 36
Processors per node: 2
Nodes: 1
Notes: Powered by the KRAI X and KILT technologies
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/systems/sr670_q4_ultra.json"> Lenovo ThinkSystem SR670 V2 (4x QAIC100 Ultra) </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol"> QUALCOMM Cloud AI 100 Ultra x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/sr670_q4_ultra/bert-99/offline"> 5996.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/sr670_q4_ultra/bert-99/offline"> 7286.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/sr670_q4_ultra/bert-99.9/offline"> 3501.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/sr670_q4_ultra/bert-99.9/offline"> 4140.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/sr670_q4_ultra/retinanet/offline"> 3002.1 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/sr670_q4_ultra/retinanet/offline"> 3489.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/sr670_q4_ultra/resnet50/offline"> 210000.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Lenovo/results/sr670_q4_ultra/resnet50/offline"> 215016.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0080 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 72
Processors per node: 1
Nodes: 1
Notes: NVIDIA MGX Reference Platform;
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT.json"> NVIDIA GH200-GraceHopper-Superchip (1x GH200-144GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA GH200-GraceHopper-Superchip x 1 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Offline"> 12.3 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Offline"> 3871.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0081 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: H200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/H200-SXM-141GB-CTSx1_TRT.json"> NVIDIA H200 (1x H200-SXM-141GB-CTS, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB-CTS x 1 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GB-CTSx1_TRT/llama2-70b-99.9/Offline"> 12.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GB-CTSx1_TRT/llama2-70b-99.9/Offline"> 4020.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0082 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: H200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/H200-SXM-141GB-CTSx8_TRT.json"> NVIDIA H200 (8x H200-SXM-141GB-CTS, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB-CTS x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GB-CTSx8_TRT/llama2-70b-99.9/Offline"> 100.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GB-CTSx8_TRT/llama2-70b-99.9/Offline"> 31712.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0083 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/H200-SXM-141GBx1_TRT.json"> NVIDIA H200 (1x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 1 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx1_TRT/gptj-99/Offline"> 29.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx1_TRT/gptj-99/Offline"> 30.8 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx1_TRT/gptj-99.9/Offline"> 29.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx1_TRT/gptj-99.9/Offline"> 30.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0084 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 9.3.0, CUDA 12.2
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/systems/H200-SXM-141GBx8_TRT.json"> NVIDIA H200 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 90.4 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 27738.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 238.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99/Offline"> 241.3 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 238.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99.9/Offline"> 241.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/stable-diffusion-xl/Offline"> 13.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/stable-diffusion-xl/Offline"> 13.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 530116.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 572867.0 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Offline"> 340028.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Offline"> 359104.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0085 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9554 64-Core Processor
Software: QUALCOMM Cloud AI SDK v1.12.2
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: Powered by the KRAI X and KILT technologies
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/systems/g293_q16_ultra_ee.json"> GIGABYTE G293-Z43 (16x QAIC100 Ultra, EE) </a> </td>
        <td class="col-submitter headcol"> Qualcomm </td>
        <td class="col-accelerator headcol"> QUALCOMM Cloud AI 100 Ultra x 16 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_ee/bert-99/offline"> 26495.2 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_ee/bert-99/offline"> 30022.2 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_ee/bert-99.9/offline"> 14120.5 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_ee/bert-99.9/offline"> 16511.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_ee/retinanet/offline"> 12495.8 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_ee/retinanet/offline"> 15105.1 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_ee/resnet50/offline"> 575143.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_ee/resnet50/offline"> 887855.0 </a> </td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 4.0-0086 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9554 64-Core Processor
Software: QUALCOMM Cloud AI SDK v1.12.2
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: Powered by the KRAI X and KILT technologies
        "> <a target="_blank" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/systems/g293_q16_ultra_pp.json"> GIGABYTE G293-Z43 (16x QAIC100 Ultra, PP) </a> </td>
        <td class="col-submitter headcol"> Qualcomm </td>
        <td class="col-accelerator headcol"> QUALCOMM Cloud AI 100 Ultra x 16 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_pp/bert-99/offline"> 27694.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_pp/bert-99/offline"> 30965.7 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_pp/bert-99.9/offline"> 14245.7 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_pp/bert-99.9/offline"> 16468.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_pp/retinanet/offline"> 13745.9 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8, fp16" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_pp/retinanet/offline"> 15476.6 </a> </td>

                        <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_pp/resnet50/offline"> 625167.0 </a> </td>

                <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v4.0/tree/main/closed/Qualcomm/results/g293_q16_ultra_pp/resnet50/offline"> 902482.0 </a> </td>

                <td></td>

                <td></td>

        </tr>
        </table></div>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<hr>

<h2>Count of Results </h2>

    <div class="counttable_wrapper">
    <table class="tablesorter counttable" id="results_summary">
    <thead>
    <tr>
    <th class="count-submitter">Submitter</th>
        <th id="col-llama2-99">LLAMA2-70B-99</th>
        <th id="col-llama2-99.9">LLAMA2-70B-99.9</th>
        <th id="col-gptj-99">GPTJ-99</th>
        <th id="col-gptj-99.9">GPTJ-99.9</th>
        <th id="col-bert-99">Bert-99</th>
        <th id="col-bert-99.9">Bert-99.9</th>
        <th id="col-dlrm-v2-99">Stable Diffusion</th>
        <th id="col-dlrm-v2-99">DLRM-v2-99</th>
        <th id="col-dlrm-v2-99.9">DLRM-v2-99.9</th>
        <th id="col-retinanet">Retinanet</th>
        <th id="col-resnet50">ResNet50</th>
        <th id="col-3d-unet-99">3d-unet-99</th>
        <th id="col-3d-unet-99.9">3d-unet-99.9</th>
        <th id="all-models">Total</th>
        </tr>
        </thead>
        <tr><td class="count-submitter"> ASUSTeK </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 40 </td></tr><tr><td class="count-submitter"> Azure </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 6 </td></tr><tr><td class="count-submitter"> CTuning </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td></tr><tr><td class="count-submitter"> Cisco </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td></tr><tr><td class="count-submitter"> Dell </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td><td class="col-result"> 14 </td><td class="col-result"> 12 </td><td class="col-result"> 22 </td><td class="col-result"> 16 </td><td class="col-result"> 16 </td><td class="col-result"> 4 </td><td class="col-result"> 6 </td><td class="col-result"> 22 </td><td class="col-result"> 20 </td><td class="col-result"> 10 </td><td class="col-result"> 11 </td><td class="col-result"> 161 </td></tr><tr><td class="count-submitter"> Fujitsu </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 18 </td></tr><tr><td class="count-submitter"> GigaComputing </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 22 </td></tr><tr><td class="count-submitter"> Google </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 20 </td></tr><tr><td class="count-submitter"> HPE </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td><td class="col-result"> 8 </td><td class="col-result"> 3 </td><td class="col-result"> 3 </td><td class="col-result"> 34 </td></tr><tr><td class="count-submitter"> Intel </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 1 </td><td class="col-result"> 11 </td></tr><tr><td class="count-submitter"> Intel-HabanaLabs </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td></tr><tr><td class="count-submitter"> Krai </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td></tr><tr><td class="count-submitter"> Lenovo </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 16 </td></tr><tr><td class="count-submitter"> NVIDIA </td><td class="col-result"> 0 </td><td class="col-result"> 16 </td><td class="col-result"> 14 </td><td class="col-result"> 14 </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 12 </td><td class="col-result"> 12 </td><td class="col-result"> 12 </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 3 </td><td class="col-result"> 3 </td><td class="col-result"> 110 </td></tr><tr><td class="count-submitter"> Oracle </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 1 </td><td class="col-result"> 1 </td><td class="col-result"> 22 </td></tr><tr><td class="count-submitter"> Quanta_Cloud_Technology </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 8 </td><td class="col-result"> 4 </td><td class="col-result"> 6 </td><td class="col-result"> 4 </td><td class="col-result"> 6 </td><td class="col-result"> 2 </td><td class="col-result"> 8 </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 2 </td><td class="col-result"> 3 </td><td class="col-result"> 57 </td></tr><tr><td class="count-submitter"> Supermicro </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 4 </td><td class="col-result"> 6 </td><td class="col-result"> 2 </td><td class="col-result"> 3 </td><td class="col-result"> 35 </td></tr><tr><td class="count-submitter"> Wiwynn </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 1 </td><td class="col-result"> 9 </td></tr><tr><td class="count-submitter"> Qualcomm </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 16 </td></tr>
    <tr>
    <td class="count-submitter">Total</td>
    <td class="col-result"> 0 </td><td class="col-result"> 44 </td><td class="col-result"> 60 </td><td class="col-result"> 44 </td><td class="col-result"> 72 </td><td class="col-result"> 50 </td><td class="col-result"> 60 </td><td class="col-result"> 28 </td><td class="col-result"> 40 </td><td class="col-result"> 70 </td><td class="col-result"> 72 </td><td class="col-result"> 27 </td><td class="col-result"> 32 </td><td class="col-result"> 599 </td></tr></table></div>
<hr>

    <div id="submittervssubmissionchartContainer" class="bgtext" style="height:370px; width:80%; margin:auto;"></div>
    <div id="modelvssubmissionchartContainer" class="bgtext" style="height:370px; width:80%; margin:auto;"></div>

    <form id="resultSelectionForm" method="post" action="">
        <h3>Select Category and Division</h3>

        <div class="form-field">
            <label for="category">Category</label>
            <select id="category" name="category" class="col">
                <option value='datacenter' >Datacenter</option>
<option value='edge' >Edge</option>

            </select>
        </div>

        <div class="form-field">
            <label for="division">Division</label>
            <select id="division" name="division" class="col">
                <option value='closed' >Closed</option>
<option value='open' >Open</option>

            </select>
        </div>

        <div class="form-field">
            <label for="with_power">Power</label>
            <select id="with_power" name="with_power" class="col">
                <option value="true" >Performance and Power</option>
                <option value="false" selected>Performance</option>
            </select>
        </div>

        <div class="form-field">
            <button type="submit" name="submit" value="1" id="results_tablesorter">Submit</button>
        </div>
    </form>


<script type="text/javascript">
var sortcolumnindex = 4, perfsortorder = 1;
</script>

<!--<script type="text/javascript" src="javascripts/tablesorter.js"></script>-->
<script type="text/javascript" src="javascripts/results_tablesorter.js"></script>
<script type="text/javascript" src="javascripts/results_charts.js"></script>

</html>







  
  




  



                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": ".", "features": ["content.tabs.link", "content.code.copy", "navigation.expand", "navigation.sections", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.follow"], "search": "assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
  
      <script src="assets/javascripts/bundle.af256bd8.min.js"></script>
      
    
  <script type"text/javascript">
  var resort = true, // re-apply the current sort
        callback = function() {
          // do something after the updateAll method has completed
        };

      // let the plugin know that we made a update, then the plugin will
      // automatically sort the table based on the header settings
      $("table").trigger("updateAll", [ resort, callback ]);
  </script>


  </body>
</html>